---
title:  "[Lecture 3] Loss Functions and Optimization"

categories:
  - cs231n
tags:
  - [yai, cs231n]
 
date: 2022-01-13
last_modified_at: 2022-01-14
published: false
---

<img src="https://user-images.githubusercontent.com/86605720/149466606-ff0a85af-38d9-43fc-b079-4cebdf83d3fb.png" alt="image" style="zoom:50%;" />



# 이번 시간에 배울 내용

* SVM Loss
* Regularization
* Softmax Classifier
* Optimization
* Gradient Descent

---



#  SVM Loss

* **Loss function** :training data로 얻은 score가 얼마나 잘못 예측됐는지 정량화시키는 함수

<img src="https://user-images.githubusercontent.com/86605720/149469152-96942995-fa11-4644-8c4b-72e429092d25.png" alt="image" style="zoom:50%;" />

위의 수식에서 s 는 predicted score 를 의미한다. SVM Loss는 true category의 score를 제외하고, true category의 score가 false category의 score보다 1이상 큰 경우가 아니라면 그 차이 + 1을 loss에 합해준다. 

**즉, true score가 false score보다 최소 1이상 충분히 커야 만족하는 것이다.**

* cat의 SVM loss : true score인 3.2이 car의 score인 5.1보다 크기 때문에 그 차이인 1.9에 1을 더해주고 frog의 score보다 1이상 크기 때문에 frog의 score는 고려하지 않는다.
* car의 SVM loss : true score인 4.9가 cat과 frog의 score보다 모두 1이상 크기 때문에  loss는 0이다.
* frog의 SVM loss : true score인 -3.1이 cat과 car의 score보다 작기 때문에 그 차이와 2를 합해준다.

* **safety margin** : 위 자료의 파란 박스 내의 +1 항이다. 이 값은 임의로 정한 값이다. loss의 절대적인 값보다 각 category의 score간의 상대적인 차이가 더 중요한데, w에 따라 score의 값이 바뀔텐데, score간의 상대적인 차이를 rescaling해주는 도구로 생각하면 될 것 같다.

#### Q1  : What happens to loss if car scores change a bit?

SVM loss의 주 목적은 correct score가 incorrect score에 비해 1 이상 큰지 확인하는 것인데, car의 경우 cat과 frog의 score에 비해 현저히 크기 때문에 값이 조금 바뀌어도 loss는 여전히 0 이다.

#### Q2 : what is min/max possible loss?

minimum loss : zero, maximum loss : correct score가 negatively infinite하면 loss는 positively infinite하다.

#### Q3 : At initialization W is small so all s~0. What is the loss?

일반화 하면 #class -1 이다. 이는 초기에 w를 작게 설정해서 loss 값이 0인지 확인하는 방식으로 프로그램에 버그가 있는지 찾을 수 있다.

#### Q4 : What if the sum was over all classes?

loss가 1 증가한다. 관례적으로 minimum loss를 0으로 맞춰주기 위해 포함하지 않는다.

#### Q5 : What if we used mean instead of sum?

상대적인 차이가 중요하고 평균을 쓰는 것은 상대적인 차이를 rescaling하는 것일뿐이므로 결과에 큰 영향이 없다.

#### Q6 : what if we used square loss?

square loss는 차이가 크면 클수록 loss에 더 크게 반영된다. hinge loss는 선형적으로 loss에 반영한다. 어떤 type의 error를 사용할지는 경우에 따라 선택해주면 되는 것 같다.

#### E.g. Suppose that we found a W such that L=0. Is this unique? answer is false

___



# Regularization

* **overfitting**은 overtraining으로 인해 train dataset에 대해만 잘 작동하고, 새로운 data에 대해서는 잘 작동하지 않는 경우를 말한다.  머신러닝에서 해결해야될 문제 중 하나인데, regularization으로 어느정도 방지할 수 있다.

<img src="https://user-images.githubusercontent.com/86605720/149473829-5fdedcc4-181c-4ba7-ab74-1980d3c7ab09.png" alt="image" style="zoom:50%;" />



* 위 수식에서 lambda는 앞의 항 Data loss와 뒤의 Regularization을 조율해주는 hyperparameter이다. training error는 늘어날 수 있지만 weight의 크기를 작게 만들고 complexity를 낮춰서 결과적으로 test set의 performance가 더 좋아지는 효과가 있다.

<img src="https://user-images.githubusercontent.com/86605720/149475907-e70617fe-316c-4de7-946e-a9a94e4fdaff.png" alt="image" style="zoom:50%;" />

* L2 regularization은 weight의 euclidean norm인데, 위의 경우 w2를 선호한다. L2 regularization은 w가 spread 돼서 모든 x를 고려해서 loss가 특정 x 값에 robust하게한다. 

---



# Softmax Classifier ( Multinomial Logistic Regression)

<img src="https://user-images.githubusercontent.com/86605720/149478920-65292c1a-61c0-4cbc-87eb-1b5cedd00884.png" alt="image" style="zoom:50%;" />



* 위 그림의 박스 안의 P 에 대한 함수를 **softmax function**이라고 한다.
* 위 그림의 박스 안에 L에 대한 함수를 **cross entropy loss**라고 한다.

<img src="https://user-images.githubusercontent.com/86605720/149480367-df5054ae-849b-4bb7-a64c-ae7e0a2dd0a6.png" alt="image" style="zoom:50%;" />

* softmax function을 사용하면 score의 값을 각 class의 score를 확률로 해석할 수 있다.
* 확률로 nomalize한 뒤에 loss function을 원하는 class의 score에 적용해준다.



#### Q1: What is the min/max possible loss L_i?

correct score가 1일 때, loss는 0이고, correct score가 0일 때, -log를 취하면 maximum값은 positively unbounded하다.

#### Q2: Usually at initilization W is small so all s~0. What is the loss?

-log(1/3)이다. SVM과 마찬가지로 프로그램의 버그가 있는지 확인하는 용도로 사용할 수 있다.

## Softmax vs SVM

#### Q: Suppose I take a datapoint and I jiggle a bit. What happens to the loss in both cases?

SVM은 true score가 다른 score에 비해 1이상 충분히 큰가에 관심있기 때문에, score가 조금 변해도 loss가 변하지 않는다.

softmax는 true score의 확률이 1에 가까운가에 관심 있기 때문에 loss에 민감하게 변한다.



---



# Optimization and Gradient Descent

* loss를 최소화하기 위해 최적의 weight을 구하는 과정
* random search와 numerical gradient를 이용해서 weight을 구하는 방식은 지양한다.
* analytic하게 weight에 대한 Loss의 gradient를 계산하고, gradient가 negative한 방향으로 w를 updata한다.

```python
# vanilla Gradient Descent

While True:
	weights_grad = evaluate_gradient(loss_fn, data, weights)
	weights += - step_size * weights_grad #perform parameter update
```

---



# Aside: Image Features

<img src="https://user-images.githubusercontent.com/86605720/149486986-854956d6-cc98-4065-ae1c-2c380733e007.png" alt="image" style="zoom:50%;" />

* CNN network를 사용하기 이전 전통적인 방식으로는 그림의 윗부분처럼 edge의 orientation이나 색에 대한 정보 등 여러 feature들을 따로 뽑아서 하나의 긴 vector로 concatenate해주고 train하는 과정을 거쳤다.
* 하지만 CNN network를 사용한 이후에는 feature를 인위적으로 추출하는 과정을 생략한다.
